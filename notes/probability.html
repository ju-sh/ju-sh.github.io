<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Probability</title>
  <style>
    body {
      max-width: 38rem;
      padding: 2rem;
      margin: auto;
      background-color: #FAF0E6;
    }
    table, th, td {
      border: 1px solid black;
      border-collapse: collapse;
    }
  </style>

</head>
<body>


<div id="header">
<a href="https://ju-sh.github.io">Home</a>
 | 
<a href="https://ju-sh.github.io/blog/index.html">Blog</a>
 | 
<a href="https://ju-sh.github.io/wiki/index.html">Wiki</a>
 | 
<a href="https://ju-sh.github.io/about.html">About</a>
</div>

<header id="title-block-header">
<h1 class="title">Probability</h1>
</header>

<ul>
    </ul>




<hr/>

<div id="content-container">
<ul>
<li>If an event has probability <code>p</code>, we can expect first success in <code>1/p</code> trials</li>
</ul>
<h2 id="bayes-theorem">Bayes' theorem</h2>
<pre><code>          P(A) * P(B/A)
P(A/B) = ──────────────
              P(B)
</code></pre>
<p>—</p>
<p>Derivation<a href="http://www.hep.upenn.edu/~johnda/Papers/Bayes.pdf">ʳ</a>:</p>
<p><code>P(A ∩ B)</code> is the probability of A times probablity of B given that A has already happened.</p>
<pre><code>P(A ∩ B) = P(A) * P(B/A)
</code></pre>
<p>It could also be defined as the probability of B times probablity of A given that B has already happened.</p>
<pre><code>P(A ∩ B) = P(B) * P(A/B)
</code></pre>
<p>Equating the two,</p>
<pre><code>P(A) * P(B/A) = P(B) * P(A/B)


                 P(A) * P(B/A)
 =&gt;    P(A/B) = ──────────────
                     P(B)
</code></pre>
<h2 id="probability-distribution-models">Probability distribution models</h2>
<h3 id="poisson-distribution">Poisson distribution</h3>
<p>Same event that happens multiple times over a time interval.</p>
<ul>
<li>Discrete</li>
<li>Parameter: mean/expectation (λ)</li>
</ul>
<p>Probability of k events (probability density/mass function):</p>
<pre><code>        λᵏ.e⁻ᵏ
P(k) = ───────
         k!
</code></pre>
<p>Poisson density function is not continuous. It's defined only for integer values of <code>k</code>.</p>
<p>See: <a href="https://brilliant.org/wiki/poisson-distribution/">https://brilliant.org/wiki/poisson-distribution/</a></p>
<h3 id="gamma-distribution">Gamma distribution</h3>
<ul>
<li>Parameters
<ul>
<li>Shape parameter k</li>
<li>Rate parameter θ</li>
</ul></li>
</ul>
<p>DBT: Mean is past the midpoint in the graph always??</p>
<h3 id="normal-distribution">Normal distribution</h3>
<ul>
<li>aka Gaussian distribution</li>
<li>Bell shaped curve</li>
<li>Continuous ??</li>
</ul>
<h3 id="categorical-distribution">Categorical distribution</h3>
<ul>
<li><p>Value of random variable is from one among a set of predefined categories <a href="https://www.statology.org/categorical-distribution/">ʳ</a></p></li>
<li><p>Each category has an associated probability</p></li>
<li><p>A generalization of Bernoulli distribution ?? <a href="https://stats.stackexchange.com/questions/113377/what-is-meant-by-categorical-distribution">ʳ</a></p></li>
<li><p>Discrete</p></li>
<li><p>Doesn't have anything to do with category theory</p></li>
</ul>
<p>Examples:</p>
<ul>
<li>Rolling of a 6-faced die.
<ul>
<li>Each category has equal probability: 1/6</li>
</ul></li>
<li>Coin toss</li>
<li>Selecting marbles from an urn <a href="https://www.statology.org/categorical-distribution/">ʳ</a>
<ul>
<li>Suppose urn has 5 red, 3 green and 2 blue marbles</li>
<li>Probabilities of categories are like:
<ul>
<li>Red: 5/10</li>
<li>Green: 3/10</li>
<li>Blue: 2/10</li>
</ul></li>
</ul></li>
</ul>
<h3 id="more">More</h3>
<ul>
<li>Exponential
<ul>
<li>For between 2 occurrences of a same event??</li>
</ul></li>
<li>Erlang</li>
<li>Binomial</li>
<li>Bernoulli</li>
<li>Uniform distribution</li>
</ul>
<h2 id="central-limit-theorem">Central limit theorem</h2>
<p>Given a collection of points (of any probability distribution. Need not be normal), if we select k number of points repeatedly with replacement (ie, the k points are considered to be 'put back' after each trial), the mean value of the trials will be normally distributed.</p>
<p>See:</p>
<ul>
<li><a href="https://www.scribbr.com/statistics/central-limit-theorem/">https://www.scribbr.com/statistics/central-limit-theorem/</a></li>
<li><a href="https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_probability/BS704_Probability12.html">https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_probability/BS704_Probability12.html</a></li>
</ul>
<h2 id="books">Books</h2>
<ul>
<li>Understanding Probability - Henk Tijms</li>
<li>A first course in probability - Sheldon Ross</li>
<li>MIT 6.041SC Probabilistic Systems Analysis and Applied Probability course (MIT OpenCoursWare)
<ul>
<li><a href="https://www.youtube.com/playlist?list=PLUl4u3cNGP60A3XMwZ5sep719_nh95qOe">https://www.youtube.com/playlist?list=PLUl4u3cNGP60A3XMwZ5sep719_nh95qOe</a></li>
</ul></li>
<li><a href="http://bayesianthink.blogspot.com/2012/12/the-best-books-to-learn-probability.html#.Ur8uWBVx05k">http://bayesianthink.blogspot.com/2012/12/the-best-books-to-learn-probability.html#.Ur8uWBVx05k</a></li>
<li><a href="https://projects.iq.harvard.edu/stat110">https://projects.iq.harvard.edu/stat110</a></li>
</ul>
</div>
</body>
</html>
