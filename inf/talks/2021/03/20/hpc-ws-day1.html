<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>hpc-ws-day1</title>
  <style>
    body {
      max-width: 38rem;
      padding: 2rem;
      margin: auto;
      background-color: #FAF0E6;
    }
    table, th, td {
      border: 1px solid black;
      border-collapse: collapse;
    }
  </style>

</head>
<body>


<div id="header">
<a href="https://rawtext.club/~famubu">Home</a>
 | 
<a href="https://rawtext.club/~famubu/blog/index.html">Blog</a>
 | 
<a href="https://rawtext.club/~famubu/wiki/index.html">Wiki</a>
 | 
<a href="https://rawtext.club/~famubu/about.html">About</a>
</div>


<ul>
    </ul>




<div id="content-container">
<h1 id="hpc-workshop---day-1">HPC workshop - Day 1</h1>
<ul>
<li>Speaker: Nikhil Hegde from IIT Dharwad</li>
<li>Date: 20-Mar-2021</li>
<li><a href="https://www.cse.iitm.ac.in/~rupesh/events/hpc2021/">Info
link</a></li>
<li>NSM HPC 2-day workshop</li>
</ul>
<p>Couldn’t actively participate in the lab session (didn’t have access
to a cluster to try it out).</p>
<hr />
<p>Chandra: An HPC cluster at IITPKD.</p>
<p>The giant computers consists of many smaller computes linked
together.</p>
<h1 id="basic-terminology">Basic terminology</h1>
<h2 id="flops">FLOPS</h2>
<ul>
<li>Floating-point operations per second.</li>
<li>aka flop/s</li>
</ul>
<h2 id="node">Node</h2>
<p>Standalone computer in the HPC cluster.</p>
<h2 id="master-login-node">Master / login node</h2>
<p>The one with which the user interacts directly.</p>
<h2 id="cpuprocessor-and-sockets">CPU/Processor and sockets</h2>
<p>Socket: Some vendors term multicore CPU sockets.</p>
<h2 id="virtual-cpu">Virtual CPU</h2>
<p>A VM assigned to a single CPU core.</p>
<h2 id="interconnect">Interconnect</h2>
<ul>
<li><strong>Cabling</strong> and associated devices to connect the
different components of the HPC cluster together.</li>
<li>Cabling determines the <strong>topology</strong>.</li>
</ul>
<h2 id="storage">Storage</h2>
<p>###$HOME Where you land upon logging in</p>
<h3 id="scratch">$SCRATCH</h3>
<ul>
<li>not just an env var</li>
<li>a faster memory where we keep all data needed for executing the
task</li>
<li>Eg: Lustre, BeeGFS</li>
</ul>
<h3 id="tmp-optional">/tmp (optional)</h3>
<p>Storage may be available in individual nodes as well.</p>
<h3 id="storage-servers">Storage servers</h3>
<p>Dedicated for storage</p>
<h2 id="batch-system">Batch system</h2>
<ul>
<li>Consists of a job scheduler and resource manager.</li>
<li>Provides user interface to submit, run and monitor jobs</li>
</ul>
<h3 id="slurm">Slurm</h3>
<ul>
<li>Slurm workload manager.</li>
<li>Formerly stood for Simple Linux Utility for Resource Management</li>
</ul>
<h3 id="pbs">PBS</h3>
<ul>
<li>Portable Batch System</li>
<li>A job scheduler like SLURM. ie, a batch system.</li>
</ul>
<h3 id="platform-lsf">Platform LSF</h3>
<p>Platform Load Sharing Facility</p>
<h2 id="paritionsqueues">Paritions/queues</h2>
<ul>
<li>Logical grouping of some nodes in the cluster.</li>
<li>Single node can theoretically belong to multiple partitions but this
is not usually practiced.</li>
<li>Defines attributes and limits for a job submitted to this
partition.</li>
</ul>
<h2 id="software">Software</h2>
<p>ICC, MPICH, NVProf, Intel Parallel Studio, etc.</p>
<h2 id="cluster">Cluster</h2>
<p>Consists of - Master node (front-end) - Back-end - Interconnect -
File system ($HOME, $SCRATCH, etc)</p>
<p>Log in at Master node (possibly using SSH).</p>
<h1 id="linux-commands">Linux commands</h1>
<p>scp : to copy files between a remote server and your computer.</p>
<h1 id="life-of-a-job">Life of a job</h1>
<h2 id="job-creation">Job creation</h2>
<ul>
<li>Max num of nodes,cores needed</li>
<li>Max amount of mem, time needed</li>
<li>Commands involved</li>
</ul>
<h2 id="job-submission">Job submission</h2>
<p>Means, - No syntax errors - Amount of requested resources can
possibly be allocated. - Job object created in the queue in which it is
to executed. - Whether exclusive access to the core is needed. - A job
id is returned.</p>
<h2 id="job-execution">Job execution</h2>
<ul>
<li>Requested resources becomes available.</li>
<li>From here on, you can’t request more resources.</li>
<li>Commands specified are executed one by one.</li>
</ul>
<h2 id="job-monitoring">Job monitoring</h2>
<ul>
<li>Can delete the job if needed.</li>
<li>Check job status.</li>
</ul>
<h2 id="job-completion">Job completion</h2>
<ul>
<li>Normal or abnormal completion</li>
<li>May also be because the job exceeded its stated resource/time
requirement.</li>
</ul>
<h1 id="programming-clusters">Programming clusters</h1>
<p>All about taking advantage of the parallelism.</p>
<h2 id="process">Process</h2>
<ul>
<li>Self contained.</li>
<li>Has its own share of resources allocated to it.</li>
<li>An illusion that it owns an entire computer for itself</li>
</ul>
<h2 id="threads">Threads</h2>
<ul>
<li>Belongs to a process</li>
<li>Shares resources with other threads of the process.</li>
<li>An illusion that it owns an entire processor core for itself?</li>
</ul>
<h2 id="kinds-of-execution">Kinds of execution</h2>
<h3 id="sequential-program">Sequential program</h3>
<h3 id="concurrent-program">Concurrent program</h3>
<h4 id="multiprogramming">Multiprogramming</h4>
<p>Threads muxing their execution on a single processor. ####
Multiprocessing Threads muxing their execution on multiple cores. ####
Distributed computing Threads muxing their execution on multiple
nodes.</p>
<h2 id="parallel-program">Parallel program</h2>
<p>Program designed to execute on hardware supporting parallelism.</p>
<h2 id="flynns-taxonomy">Flynn’s taxonomy</h2>
<ul>
<li>Categories of computing systems</li>
<li>Based on how processors see instructions and data.</li>
</ul>
<h3 id="sisd-single-instruction-single-data">SISD (Single Instruction
Single Data)</h3>
<p>Single core computer + single stream of data</p>
<h3 id="simd-single-instruction-multiple-data">SIMD (Single Instruction
Multiple Data)</h3>
<ul>
<li>Parallel hardware</li>
<li>Same instruction but multiple ‘slices’ of data are processed at the
same time.</li>
<li>All cores execute the same instruction in the same clock cycle but
operate on different data.</li>
<li>Eg: GPUs, modern CPUs with SIMD components.</li>
</ul>
<h3 id="misd-multiple-instruction-single-data">MISD (Multiple
Instruction Single Data)</h3>
<p>Not that common? Not sure…</p>
<h3 id="mimd-multiple-instruction-multiple-data">MIMD (Multiple
Instruction Multiple Data)</h3>
<ul>
<li><p>Clusters belong in the MIMD category.</p></li>
<li><p>Most common type of parallel computer</p></li>
<li><p>Cores may be operating in sync or async</p></li>
<li><p>Eg: clusters, supercomputers, multi-core CPUs.</p></li>
<li><p>Clusters are specialized supercomputers?</p></li>
<li><p>Clusters =&gt; different nodes may be having different
OS.</p></li>
</ul>
<h2 id="parallel-hardware">Parallel hardware</h2>
<p>Categorization based on hardware - Shared memory architecture: all
processors share the same mem - Distributed memory arch: each processor
has some mem allocated to it. (but are these mem or processor that are
connected??)</p>
<h2 id="data-parallelism">Data parallelism</h2>
<p>Threads execute the same function but operate on different data.</p>
<h2 id="controltask-parallelism">Control/task parallelism</h2>
<p>Like carpenter getting the window ready when the mason is building
the wall.</p>
<h2 id="openmp">OpenMP</h2>
<ul>
<li>For shared memory programming.</li>
<li>C/C++/Fortran</li>
<li>Single Program Multiple Data (SPMD)</li>
<li>Seamless automatic scaling: Can add more processors without having
to modify the programs. This wasn’t possible with pthreads??</li>
<li>Scalable, portable alternative to data-parallel computing on shared
memory architectures.</li>
</ul>
<h2 id="programming-in-openmp">Programming in OpenMP</h2>
<pre><code>// executes as many threads as there are processors (possible to change this number)
#pragma omp parallel 
// Divides a for loop
#pragma omp for </code></pre>
<p>Like (not sure if correct),</p>
<pre><code>#pragma omp parallel 
{
    int sum = 0;
    #pragma omp for 
    for(int i=0; i&lt;100; ++i)
    {
        sum += i;
    }
}</code></pre>
<ul>
<li><p>Programmer still responsible for handling data races.</p></li>
<li><p>Execution begins with a <strong>master thread</strong>.</p></li>
<li><p>Master thread spawns worker threads.</p></li>
<li><p>Worker threads ‘joins’ master thread (after execution?)</p>
<p>omp_set_num_threads(P) // set number of threads</p></li>
</ul>
<h2 id="non-determinism">Non-determinism</h2>
<p>The exact order of instructions executed is not deterministic (not a
total ordering, but a parital ordering?).</p>
<h3 id="example-sum-of-an-int-array">Example: Sum of an int array</h3>
<pre><code>sum = 0  // set by master</code></pre>
<p>sum is shared among workers. Only worker should set it at a time
=&gt; possible <strong>data race</strong>.</p>
<p>Alternatively, make two variables <code>sum1</code> and
<code>sum2</code> so that two independent courses of action can
proceed.</p>
<p>Master joins the result (the partial results computed by the worker
threads) at the end.</p>
<h2 id="distributed-memory-programming">Distributed memory
programming</h2>
<ul>
<li>Program executes as a collection of processes.</li>
<li>Each process/processor has its own memory.</li>
<li>Total memory available = sum of that of all processes.</li>
<li>Data exchange between processes =&gt; processes need to cooperate.
ie, explicit communication.</li>
<li>Every data element belongs to the memory of one of these
processes.</li>
<li>If a data element is placed in the memory of a process which barely
uses it may mean avoidable communication overhead. =&gt; more complex
than a shared memory architecture.</li>
<li>Most programs are written in SPMD model.</li>
<li>Computing power and cost scaling better than shared memory. (because
of stuff like rack-mounted blades?)</li>
</ul>
<h2 id="an-example-c-program-using-mpi">An example C program using
MPI</h2>
<pre><code>#include&lt;mpi.h&gt;

int main(int argc, char *argv[]) {
    // Initialize MPI env
    MPI_Init(&amp;argc, &amp;argv);

    // rest of the code

    // Release system resources
    MPI_Finalize();
}</code></pre>
<p>We got to divide the work first.</p>
<p>We can use the values of rank and size for this.</p>
<h3 id="ranks-in-mpi">Ranks in MPI</h3>
<ul>
<li>Ranges between 0 to 5. ie, ∈ [0,5]</li>
<li>Like a process number?</li>
</ul>
<h3 id="size-in-mpi">Size in MPI</h3>
<p>Number of processes in the execution environment?</p>
<h2 id="a-very-rough-depiction-of-some-mpi-commands">A very rough
depiction of some MPI commands</h2>
<pre><code>MPI_Send(send_buffer, count of sent items, data type of sent items, dest process id, message id)
MPI_Receive(receive buffer, count of received items, any message id (for source??), from any process id, )

// Aggregate results
MPI_Gather(send_buffer, count, datatype, receive_buffer (from master?), receive_count, receive_type, ...)</code></pre>
<h1 id="git-repo">Git repo</h1>
<p><a
href="https://github.com/IITdhtraining/hpc101">https://github.com/IITdhtraining/hpc101</a></p>
<h1 id="mpi-and-openmp-difference">MPI and OpenMP difference</h1>
<ul>
<li>MPI for shared memory architecture</li>
<li>OpenMP for distributed memory architecture (distributed
computing)</li>
<li>Hybrid systems possible. ie, same system could use both MPI and
OpenMP.</li>
</ul>
<h1 id="interesting">Interesting</h1>
<ul>
<li><code>mpic++ prog.cpp</code></li>
<li><code>mpirun -n10 helloworld  # Create 10 copies of this project</code></li>
<li>valgrind and cachegrind</li>
<li>gprof</li>
<li>opanacc</li>
<li>openmp</li>
</ul>
</div>
</body>
</html>
